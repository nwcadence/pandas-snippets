{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample a very large CSV file using pandas\n",
    "\n",
    "The following shows two ways to sample a large CSV file to get the desired number of rows as a subset.\n",
    "This is useful for getting a sample for understanding the data, and doing fast manipulations prior to applying logic across an entire file.\n",
    "\n",
    "The two approaches are as follows:\n",
    "1) Get the first x rows in a file - very, very fast, but not random\n",
    "2) Randomly select x rows from the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Select the sample size, in rows, that we want\n",
    "sample_size = 5000\n",
    "\n",
    "# define input and output file names\n",
    "huge_file_name = 'hugefile.csv'\n",
    "first_x_rows_file_name = 'firstxrows.csv'\n",
    "random_sample_file_name = 'randomsample.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the first x rows in a file\n",
    "\n",
    "This is very, very fast and simple. But the data will likely not represent the overall dataset appropriately. That may not be a problem, but it depends strongly on what you'll be doing with the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the CSV data, passing in nrows as a parameter to get just that number of rows (from the beginning)\n",
    "df = pd.read_csv(huge_file_name, sep=\",\", low_memory=False, nrows=sample_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the partial sample as CSV\n",
    "df.to_csv(first_x_rows_file_name, sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly select x rows in a file\n",
    "\n",
    "This is very, very slow with pandas and not as simple. But the data will be more likely to represent the actual dataset. \n",
    "\n",
    "The problem is that pandas.read_csv only has a skiprows parameter, not a takerows parameter.  Which means that you have to create a massive array of random numbers for lines to skip (that don't have duplicates) to get a small number of rows.  For instance, in my current dataset I have approximately 20 million rows.  Of which I want to pull out only 5000.  So I have to create an array of 1,995,000 random numbers. This will define the rows NOT taken.\n",
    "\n",
    "Thanks to http://stackoverflow.com/users/2643104/queise for the idea on StackOverflow: http://stackoverflow.com/questions/22258491/read-a-small-random-sample-from-a-big-csv-file-into-a-python-data-frame\n",
    "\n",
    "There are other, faster and potentially cleaner, ways to get a small subset from a document.\n",
    "\n",
    "1) Convert to HDF5 and us pandas.read_hdf method, which allows for sampling directly: http://stackoverflow.com/questions/21039772/pytables-read-random-subset\n",
    "\n",
    "2) Read through the file and randomly select rows to add to another file in pure Python. http://stackoverflow.com/questions/10819911/read-random-lines-from-huge-csv-file-in-python\n",
    "\n",
    "However, I stuck with the pandas.read_csv approach because it's only run rarely, so I don't care if it takes a long time. This isn't part of a pipeline for me, just a one-off every now and again to grab data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:03:33.943099\n",
      "18:04:28.113050\n",
      "20841012\n"
     ]
    }
   ],
   "source": [
    "# First determine the number of rows in the file\n",
    "#   Iterate through all rows and sum\n",
    "#   Also, time how long it takes, just for FYI\n",
    "print(datetime.datetime.now().time()) # start time\n",
    "with open(huge_file_name) as f:\n",
    "    total_rows = sum(1 for _ in f)\n",
    "print(datetime.datetime.now().time()) # end time\n",
    "print(total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows to skip: 20836012\n",
      "18:11:06.165123\n",
      "18:11:41.467551\n",
      "Number of rows in DataFrame: \n"
     ]
    }
   ],
   "source": [
    "# create a list of size = num_rows - sample_size of random numbers\n",
    "#   I'm always including the first row, since I'm making the assumption that contains header information\n",
    "lines2skip = np.random.choice(np.arange(1,total_rows+1), (total_rows-sample_size), replace=False)\n",
    "print(\"Number of rows to skip: {0}\".format(len(lines2skip)))\n",
    "\n",
    "print(datetime.datetime.now().time()) # start time\n",
    "df = pd.read_csv(huge_file_name, sep=\",\", low_memory=False, skiprows=lines2skip)\n",
    "print(datetime.datetime.now().time()) # end time\n",
    "print(\"Number of rows in DataFrame: {0}\".format(len(df.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the partial sample as CSV\n",
    "df.to_csv(random_sample_file_name, sep=\",\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
